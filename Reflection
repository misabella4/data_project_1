# Reflection
Overall, I think this project was a great real-world application of what we've been learning in DS 2002, and it has been very helpful to work on using a combination of Python, SQL, pandas, etc to better understand the course as a whole. It was very cool to set up a Spotify Developer account and use trial and error (and a little help from AI) to get the data to do what I wanted it to. I think although I faced some challenges, especially in the beginning when setting up, I will take what I learned from this project and it apply to future job opportunities / portfolios in data-driven roles. The main struggle I dealt with was taking the entire assignment and breaking it down into more managable parts. I have a hard time planning out my work before I begin, as I'm very hands-on and eager to begin. I think this is a pretty hefty project in general, and due to the use of so many different sources, I had to learn as I went that it's important to stop and reflect as I'm conquering each step. Knowing and understanding what was actually being asked of me also helped me to micromanage each piece of my project, working to perfect the specific section I was focused on. This helped me make sure nothing broke later on in my coding, which was extremely helpful towards the end! The main aspect that was much easier than expected was turning my merged df into a SQL table. With just a simple Python function, I was able to use the magic of the sqlite3 import to turn already organized data into a corresponding SQL table. Because I'm more familiar with SQL querying and table creation anyways, it was a nice break doing this section of my project as it felt much less tedious and more enjoyable. The most difficult aspect in my opinion was accounting for the user's choice in input, since many of the functions and "data dumping" code I had to include was unfamiliar. Luckily, I made sure to budget enough time to successfully research via StackOverflow and our previous assignments, which allowed me to truly understand every method or function I was using in my own code. Once I got this down, it was nice to use AI to help simplify it and explain the specific parts I was still having a hard time grasping. Like we said in class, GPT can be your friend if you a) know how to use it properly, and b) are learning / actually understanding what you're putting into it. I think in general this type of utility could be extremely useful for future data projects, especially when you're working to build something that actually comes from various data sources. Knowing now first-hand how to accumulate data from both APIs and external files, and being able to merge / use them effectively is huge. It definitely makes me want to keep challenging myself to doing more data projects like this, and finding different data sets and correlations that could be useful in this way.  
